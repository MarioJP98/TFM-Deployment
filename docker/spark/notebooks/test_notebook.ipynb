{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23eb1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "import numpy as np\n",
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a039cfbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Localiza un archivo de canción\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m filename = \u001b[43mglob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mTRAAAAW128F429D538.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Abre el archivo\u001b[39;00m\n\u001b[32m      7\u001b[39m h5 = GETTERS.open_h5_file_read(filename)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Localiza un archivo de canción\n",
    "filename = glob.glob(\n",
    "    r'dataset\\A\\A\\A\\TRAAAAW128F429D538.h5'\n",
    ")[0]\n",
    "\n",
    "# Abre el archivo\n",
    "h5 = GETTERS.open_h5_file_read(filename)\n",
    "\n",
    "# Obtén algunos datos\n",
    "artist_name = GETTERS.get_artist_name(h5)\n",
    "song_title = GETTERS.get_title(h5)\n",
    "year = GETTERS.get_year(h5)\n",
    "duration = GETTERS.get_duration(h5)\n",
    "\n",
    "print(\n",
    "    f\"Artist: {artist_name}, Song: {song_title}, Year: {year}, Duration: {duration:.2f}s\")\n",
    "\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739600f",
   "metadata": {},
   "source": [
    "#### 1. Crear la sesión con SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d476e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 12:11:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize spark:\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# print(pyspark.__version__)\n",
    "# findspark.init(\"C:\\spark\\spark-3.5.1-bin-hadoop3\\spark-3.5.1-bin-hadoop3\")\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MSD Recommender\") \\\n",
    "#     .master(\"spark://192.168.1.130:7077\") \\\n",
    "#     # .config(\"spark.driver.host\", \"192.168.1.130\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"12g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"3\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MillionSongProcessing\") \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('XML ETL') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('job.local.dir', 'file:/models/music-recommender-model') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(10).toDF(\"number\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75240a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5647417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.range(10)  # 1 millón de filas\n",
    "print(df_test.count())  # Debe devolver 1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fc4ff",
   "metadata": {},
   "source": [
    "#### Anañizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf11b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "import numpy as np\n",
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd5e8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGLA128F149C2EB.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGYE128F42B63AB.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGUL128F42ADE30.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGRK128F422CA7F.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGFE128E0785AC0.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGRU12903CAAA2D.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGTE128F424ECBC.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGWQ128F42A88CF.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGBF128F425E4D1.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGOT128F932DC65.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/G/TRBGGOS128F9307FC5.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJZN12903D0D2FC.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJFN128F429AB79.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJYY128F930DAEE.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJYR128F42371AB.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJOG12903CC3E93.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJCT128F933A606.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJXW128F42AE7D8.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJDX12903CEC5C5.h5', '/opt/bitnami/spark/datasets/millionsongsubset/B/G/J/TRBGJMS12903CE5583.h5']\n"
     ]
    }
   ],
   "source": [
    "# Cargar ruta del dataset\n",
    "\n",
    "# #En local\n",
    "# dataset_path = \"/opt/bitnami/spark/datasets/MillionSongSubset\"\n",
    "#En Empresa\n",
    "dataset_path = \"/opt/bitnami/spark/datasets/\"\n",
    "file_paths = glob.glob(f\"{dataset_path}/**/*.h5\", recursive=True)\n",
    "#Usando solo 3 paths\n",
    "file_paths = file_paths[:20]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51183695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "|  tempo|loudness| duration| key|mode|time_signature|           song_id|     timbre_mean_0|      timbre_std_0|     timbre_mean_1|      timbre_std_1|     timbre_mean_2|      timbre_std_2|      timbre_mean_3|      timbre_std_3|      timbre_mean_4|      timbre_std_4|      timbre_mean_5|      timbre_std_5|     timbre_mean_6|      timbre_std_6|      timbre_mean_7|      timbre_std_7|     timbre_mean_8|      timbre_std_8|     timbre_mean_9|      timbre_std_9|     timbre_mean_10|     timbre_std_10|     timbre_mean_11|     timbre_std_11|\n",
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "| 89.912|  -7.673| 262.3473|11.0| 0.0|           4.0|SOCRVCC12A6D4FB91B|39.290916500994065| 6.485157574829151|18.476805168986097| 68.57053457998053|-51.62657057654082| 54.74872126573452| 23.303193836978107| 71.24440430359255|-3.9088608349900547|  38.9276746809276| 10.116880715705758| 54.19194398185478| 9.424114314115311|34.492493388085045|  8.995999005964231|36.715914526156745|0.9187654075546748|26.372634728323128| 4.105624254473157|28.651694163649097|-1.4176719681908552|32.480091927946596|-1.5765795228628203|18.746554716870577|\n",
      "|125.787| -10.118|386.79465| 8.0| 1.0|           3.0|SOAPTWU12A8C1409CB|43.543822099447475| 4.856219680691549| -6.46216906077348|36.887481304729654| 26.59680220994473| 37.86246647208591|  5.250003314917119| 31.96925107978289|-19.890121546961304|29.959488181286996| -6.504622099447517| 32.09197188828048|5.1103933701657525|23.256249738332045|-0.9994718232044209|17.301505160899477|-4.467264088397786|19.364498471875645|-7.909058563535917|14.564358020757087|-0.8960375690607744|18.933847260055117| 3.1749160220994517|14.572649701069277|\n",
      "|155.979|  -6.252|228.33587| 3.0| 0.0|           4.0|SOELHNO12A8C13F09F| 47.72390584028608| 4.537994951326135|  51.8845387365912|38.218126609852774|-6.883082240762811|36.467124486434194|-11.397628128724675|29.603617480645447|  12.53478426698452|26.915178705976963|-16.691928486293186|24.324548692206207|-5.410451728247911|23.156827987716174|-1.4029261025029773| 21.00263093983742|2.5012538736591177| 16.41098939793031| 5.785213349225269|14.492584377007589|-0.7003945172824788|14.153400757390521| 0.8839642431466047| 18.43541566716436|\n",
      "|116.338|  -2.421|  67.3171| 9.0| 0.0|           1.0|SOVBLUQ12A81C20920|  53.6958947368421| 4.263183477885119| 47.21755263157894| 39.78277507312881|45.086671052631594| 40.67202815992575| -4.698179824561405|23.870819855942443|  7.451530701754385| 26.74872940389397|-27.239030701754366|22.330346350145987|12.735745614035096| 23.19066596859319| 5.6315614035087735|15.090251075094388|21.957846491228068|22.645124579812034| 15.07631578947369|15.029482122029934| -4.104600877192981|13.118548347944897| 1.0072324561403514|15.842598602950515|\n",
      "| 95.075|  -3.698|129.59302| 9.0| 0.0|           7.0|SOGDAUG12A6701F302| 53.23628291316526|1.9564314097705062|26.575246498599437| 22.36421593775795| 42.42401120448179|20.562273330701668|-1.3822801120448156| 32.90596851329996| -8.585462184873952|15.709547904666715|-25.264400560224065| 15.70661896602044| 7.268761904761906|18.691254338969305|-2.2178627450980395| 16.58541030133152| 4.933473389355741|13.529746573551552| 4.108999999999998|10.555647750307482| -2.087448179271708|11.545152146457385| -6.038739495798318|15.879562486720872|\n",
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar los features\n",
    "data = []\n",
    "metadata_list = []  # OJO cambiamos el nombre de la lista de metadatos\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "        segments_timbre = GETTERS.get_segments_timbre(h5)\n",
    "        if segments_timbre is None or len(segments_timbre) == 0:\n",
    "            timbre_mean = [0.0] * 12\n",
    "            timbre_std = [0.0] * 12\n",
    "        else:\n",
    "            timbre_mean = np.mean(segments_timbre, axis=0).tolist()\n",
    "            timbre_std = np.std(segments_timbre, axis=0).tolist()\n",
    "\n",
    "        features = {\n",
    "            'tempo': float(GETTERS.get_tempo(h5)),\n",
    "            'loudness': float(GETTERS.get_loudness(h5)),\n",
    "            'duration': float(GETTERS.get_duration(h5)),\n",
    "            'key': float(GETTERS.get_key(h5)),\n",
    "            'mode': float(GETTERS.get_mode(h5)),\n",
    "            'time_signature': float(GETTERS.get_time_signature(h5)),\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8')\n",
    "        }\n",
    "\n",
    "        for i in range(12):\n",
    "            features[f'timbre_mean_{i}'] = timbre_mean[i]\n",
    "            features[f'timbre_std_{i}'] = timbre_std[i]\n",
    "\n",
    "        song_id = GETTERS.get_song_id(h5).decode('utf-8')\n",
    "        title = GETTERS.get_title(h5).decode('utf-8')\n",
    "        artist = GETTERS.get_artist_name(h5).decode('utf-8')\n",
    "\n",
    "        meta = {\n",
    "            'song_id': song_id,\n",
    "            'title': title,\n",
    "            'artist': artist\n",
    "        }\n",
    "\n",
    "        h5.close()\n",
    "\n",
    "        return features, meta\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el fichero: {path} --> {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Recorremos los ficheros uno a uno\n",
    "for path in file_paths:\n",
    "    result, meta = extract_features(path)\n",
    "    if result is not None:\n",
    "        data.append(result)\n",
    "    if meta is not None:\n",
    "        metadata_list.append(meta)\n",
    "\n",
    "# Convertimos las listas a pandas\n",
    "df_pd = pd.DataFrame(data)\n",
    "df_meta = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Guardamos los metadatos\n",
    "df_meta.to_csv(\"songs_metadata.csv\", index=False)\n",
    "\n",
    "# Y lo convertimos a DataFrame de PySpark\n",
    "df = spark.createDataFrame(df_pd)\n",
    "\n",
    "# Visualizamos los primeros registros\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c02db8",
   "metadata": {},
   "source": [
    "#### Entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d5b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 12:22:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/06/18 12:22:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['tempo', 'loudness', 'duration', 'key', 'mode', 'time_signature'] + \\\n",
    "               [f'timbre_mean_{i}' for i in range(12)] + \\\n",
    "               [f'timbre_std_{i}' for i in range(12)]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "\n",
    "kmeans = KMeans(featuresCol='scaled_features',\n",
    "                predictionCol='cluster_id', k=20, seed=42)\n",
    "model = kmeans.fit(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8449af34-4ecc-4809-b3e6-f25a682a88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+\n",
      "|  tempo|loudness| duration| key|mode|time_signature|           song_id|     timbre_mean_0|      timbre_std_0|     timbre_mean_1|      timbre_std_1|     timbre_mean_2|      timbre_std_2|      timbre_mean_3|      timbre_std_3|      timbre_mean_4|      timbre_std_4|      timbre_mean_5|      timbre_std_5|     timbre_mean_6|      timbre_std_6|      timbre_mean_7|      timbre_std_7|     timbre_mean_8|      timbre_std_8|     timbre_mean_9|      timbre_std_9|     timbre_mean_10|     timbre_std_10|     timbre_mean_11|     timbre_std_11|        features_vec|     scaled_features|\n",
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+\n",
      "| 89.912|  -7.673| 262.3473|11.0| 0.0|           4.0|SOCRVCC12A6D4FB91B|39.290916500994065| 6.485157574829151|18.476805168986097| 68.57053457998053|-51.62657057654082| 54.74872126573452| 23.303193836978107| 71.24440430359255|-3.9088608349900547|  38.9276746809276| 10.116880715705758| 54.19194398185478| 9.424114314115311|34.492493388085045|  8.995999005964231|36.715914526156745|0.9187654075546748|26.372634728323128| 4.105624254473157|28.651694163649097|-1.4176719681908552|32.480091927946596|-1.5765795228628203|18.746554716870577|[89.912,-7.673,26...|[2.47871656111727...|\n",
      "|125.787| -10.118|386.79465| 8.0| 1.0|           3.0|SOAPTWU12A8C1409CB|43.543822099447475| 4.856219680691549| -6.46216906077348|36.887481304729654| 26.59680220994473| 37.86246647208591|  5.250003314917119| 31.96925107978289|-19.890121546961304|29.959488181286996| -6.504622099447517| 32.09197188828048|5.1103933701657525|23.256249738332045|-0.9994718232044209|17.301505160899477|-4.467264088397786|19.364498471875645|-7.909058563535917|14.564358020757087|-0.8960375690607744|18.933847260055117| 3.1749160220994517|14.572649701069277|[125.787,-10.118,...|[3.46772755664715...|\n",
      "|155.979|  -6.252|228.33587| 3.0| 0.0|           4.0|SOELHNO12A8C13F09F| 47.72390584028608| 4.537994951326135|  51.8845387365912|38.218126609852774|-6.883082240762811|36.467124486434194|-11.397628128724675|29.603617480645447|  12.53478426698452|26.915178705976963|-16.691928486293186|24.324548692206207|-5.410451728247911|23.156827987716174|-1.4029261025029773| 21.00263093983742|2.5012538736591177| 16.41098939793031| 5.785213349225269|14.492584377007589|-0.7003945172824788|14.153400757390521| 0.8839642431466047| 18.43541566716436|[155.979,-6.252,2...|[4.3000681831848,...|\n",
      "|116.338|  -2.421|  67.3171| 9.0| 0.0|           1.0|SOVBLUQ12A81C20920|  53.6958947368421| 4.263183477885119| 47.21755263157894| 39.78277507312881|45.086671052631594| 40.67202815992575| -4.698179824561405|23.870819855942443|  7.451530701754385| 26.74872940389397|-27.239030701754366|22.330346350145987|12.735745614035096| 23.19066596859319| 5.6315614035087735|15.090251075094388|21.957846491228068|22.645124579812034| 15.07631578947369|15.029482122029934| -4.104600877192981|13.118548347944897| 1.0072324561403514|15.842598602950515|[116.338,-2.421,6...|[3.20723515534368...|\n",
      "| 95.075|  -3.698|129.59302| 9.0| 0.0|           7.0|SOGDAUG12A6701F302| 53.23628291316526|1.9564314097705062|26.575246498599437| 22.36421593775795| 42.42401120448179|20.562273330701668|-1.3822801120448156| 32.90596851329996| -8.585462184873952|15.709547904666715|-25.264400560224065| 15.70661896602044| 7.268761904761906|18.691254338969305|-2.2178627450980395| 16.58541030133152| 4.933473389355741|13.529746573551552| 4.108999999999998|10.555647750307482| -2.087448179271708|11.545152146457385| -6.038739495798318|15.879562486720872|[95.075,-3.698,12...|[2.62105143972134...|\n",
      "+-------+--------+---------+----+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e23fa",
   "metadata": {},
   "source": [
    "#### Recomendación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c52cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nueva canción ha sido asignada al cluster: 9\n",
      "+------------------+\n",
      "|           song_id|\n",
      "+------------------+\n",
      "|SOBEHXG12A8C138D22|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Supongamos que tienes ya definido:\n",
    "# model, scaler_model, assembler, df_scaled\n",
    "\n",
    "# Simulamos la nueva canción\n",
    "new_song = {\n",
    "    'tempo': 120.0,\n",
    "    'loudness': -5.0,\n",
    "    'duration': 210.0,\n",
    "    'key': 2.0,\n",
    "    'mode': 1.0,\n",
    "    'time_signature': 4.0,\n",
    "    'song_id': 'NEW_SONG_ID'\n",
    "}\n",
    "\n",
    "# Simulamos timbre features\n",
    "for i in range(12):\n",
    "    new_song[f'timbre_mean_{i}'] = np.random.uniform(-50, 50)\n",
    "    new_song[f'timbre_std_{i}'] = np.random.uniform(10, 50)\n",
    "\n",
    "# Convertimos a Spark DataFrame\n",
    "new_song_df_pd = pd.DataFrame([new_song])\n",
    "new_song_df = spark.createDataFrame(new_song_df_pd)\n",
    "\n",
    "#  Ensamblamos y escalamos usando el pipeline original\n",
    "new_song_vec = assembler.transform(new_song_df)\n",
    "new_song_scaled = scaler_model.transform(new_song_vec)\n",
    "\n",
    "# Hacemos la predicción de cluster\n",
    "new_song_pred = model.transform(new_song_scaled)\n",
    "predicted_cluster = new_song_pred.select(\n",
    "    \"cluster_id\").collect()[0][\"cluster_id\"]\n",
    "\n",
    "print(f\"La nueva canción ha sido asignada al cluster: {predicted_cluster}\")\n",
    "\n",
    "# Buscamos canciones del dataset original en el mismo cluster\n",
    "\n",
    "# Si no habías guardado los clusters en el df original:\n",
    "df_with_cluster = model.transform(df_scaled)\n",
    "\n",
    "# Filtramos por el mismo cluster\n",
    "similar_songs = df_with_cluster.filter(\n",
    "    df_with_cluster.cluster_id == predicted_cluster)\n",
    "\n",
    "# Mostramos algunas recomendaciones (por ejemplo 3)\n",
    "similar_songs.select(\"song_id\").limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e393b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              song_id                          title     artist\n",
      "8  SOBEHXG12A8C138D22  Rumour (Abstract Hip Hop Mix)  bel canto\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"songs_metadata.csv\")\n",
    "\n",
    "# Supongamos que estos son los song_ids recomendados que has obtenido de tu modelo\n",
    "recommended_song_ids = [row['song_id']\n",
    "                        for row in similar_songs.select(\"song_id\").limit(3).collect()]\n",
    "\n",
    "\n",
    "# Filtramos los metadatos para obtener nombre de la canción y artista\n",
    "recommended_songs = metadata_df[metadata_df['song_id'].isin(\n",
    "    recommended_song_ids)]\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(recommended_songs[['song_id', 'title', 'artist']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f30e57-1340-4091-8d16-11db1651e830",
   "metadata": {},
   "source": [
    "#### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cb0aeb-4053-4ec8-8da4-98415c8bc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creamos el pipeline con los stages que usaste durante el entrenamiento\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Lo ajustamos sobre el dataframe ya escalado\n",
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd466de-82ae-406b-b55e-efeea7466356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model.write().overwrite().save(\"file:///models/music-recommender-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2d496-ab86-4701-8c28-3d58d71d5d94",
   "metadata": {},
   "source": [
    "#### Cargar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97416655-e5ad-4943-a375-bbd1213edb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/18 12:08:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(\"file:///models/music-recommender-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4cf9657-20ad-4476-84be-0e5d5445ae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La canción fue asignada al cluster 19\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m predicted_cluster = predicted.select(\u001b[33m\"\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m\"\u001b[39m).collect()[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLa canción fue asignada al cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_cluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m df_with_clusters = loaded_model.transform(\u001b[43mdf_scaled\u001b[49m)\n\u001b[32m     28\u001b[39m recommendations = df_with_clusters.filter(\n\u001b[32m     29\u001b[39m     df_with_clusters.cluster_id == predicted_cluster\n\u001b[32m     30\u001b[39m ).select(\u001b[33m\"\u001b[39m\u001b[33mtrack_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33martist_name\u001b[39m\u001b[33m\"\u001b[39m).limit(\u001b[32m3\u001b[39m)\n\u001b[32m     32\u001b[39m recommendations.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Crear nueva canción con timbre simulado\n",
    "new_song = {\n",
    "    'tempo': 120.0,\n",
    "    'loudness': -5.0,\n",
    "    'duration': 210.0,\n",
    "    'key': 2.0,\n",
    "    'mode': 1.0,\n",
    "    'time_signature': 4.0,\n",
    "    'track_name': 'New Song',\n",
    "    'artist_name': 'Unknown Artist'\n",
    "}\n",
    "\n",
    "for i in range(12):\n",
    "    new_song[f'timbre_mean_{i}'] = np.random.uniform(-50, 50)\n",
    "    new_song[f'timbre_std_{i}'] = np.random.uniform(10, 50)\n",
    "\n",
    "# Convertir a DataFrame de Spark\n",
    "df_new = spark.createDataFrame(pd.DataFrame([new_song]))\n",
    "\n",
    "# Predecir cluster directamente con el pipeline\n",
    "predicted = loaded_model.transform(df_new)\n",
    "predicted_cluster = predicted.select(\"cluster_id\").collect()[0][\"cluster_id\"]\n",
    "\n",
    "print(f\"La canción fue asignada al cluster {predicted_cluster}\")\n",
    "\n",
    "df_with_clusters = loaded_model.transform(df_scaled)\n",
    "\n",
    "recommendations = df_with_clusters.filter(\n",
    "    df_with_clusters.cluster_id == predicted_cluster\n",
    ").select(\"track_name\", \"artist_name\").limit(3)\n",
    "\n",
    "recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f0dab7-bf80-45f2-8118-91b9f18bf12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               song_id           title              artist\n",
      "19  SOQBCSM12AC4687CDE  Pete's Crusade  Light Of The World\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"songs_metadata.csv\")\n",
    "\n",
    "# Supongamos que estos son los song_ids recomendados que has obtenido de tu modelo\n",
    "recommended_song_ids = [row['song_id']\n",
    "                        for row in similar_songs.select(\"song_id\").limit(3).collect()]\n",
    "\n",
    "\n",
    "# Filtramos los metadatos para obtener nombre de la canción y artista\n",
    "recommended_songs = metadata_df[metadata_df['song_id'].isin(\n",
    "    recommended_song_ids)]\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(recommended_songs[['song_id', 'title', 'artist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5f62e-98a3-4497-95a0-57adae882f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
