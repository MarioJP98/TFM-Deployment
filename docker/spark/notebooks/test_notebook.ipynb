{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a039cfbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Localiza un archivo de canción\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTRAAAAW128F429D538.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Abre el archivo\u001b[39;00m\n\u001b[1;32m      7\u001b[0m h5 \u001b[38;5;241m=\u001b[39m GETTERS\u001b[38;5;241m.\u001b[39mopen_h5_file_read(filename)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Localiza un archivo de canción\n",
    "filename = glob.glob(\n",
    "    r'dataset\\A\\A\\A\\TRAAAAW128F429D538.h5'\n",
    ")[0]\n",
    "\n",
    "# Abre el archivo\n",
    "h5 = GETTERS.open_h5_file_read(filename)\n",
    "\n",
    "# Obtén algunos datos\n",
    "artist_name = GETTERS.get_artist_name(h5)\n",
    "song_title = GETTERS.get_title(h5)\n",
    "year = GETTERS.get_year(h5)\n",
    "duration = GETTERS.get_duration(h5)\n",
    "\n",
    "print(\n",
    "    f\"Artist: {artist_name}, Song: {song_title}, Year: {year}, Duration: {duration:.2f}s\")\n",
    "\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739600f",
   "metadata": {},
   "source": [
    "#### 1. Crear la sesión con SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d476e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 10) (172.18.0.3 executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     23\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMillionSongProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://spark-master:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 10) (172.18.0.3 executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 823754013007382808, local class serialVersionUID = 3516924559342767982\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "#Initialize spark:\n",
    "from pyspark.sql import SparkSession\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark\n",
    "# print(pyspark.__version__)\n",
    "# findspark.init(\"C:\\spark\\spark-3.5.1-bin-hadoop3\\spark-3.5.1-bin-hadoop3\")\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MSD Recommender\") \\\n",
    "#     .master(\"spark://192.168.1.130:7077\") \\\n",
    "#     # .config(\"spark.driver.host\", \"192.168.1.130\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"12g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"3\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MillionSongProcessing\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.range(10).toDF(\"number\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75240a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5647417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.range(10)  # 1 millón de filas\n",
    "print(df_test.count())  # Debe devolver 1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fc4ff",
   "metadata": {},
   "source": [
    "#### Anañizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf11b33a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m sc = \u001b[43mspark\u001b[49m.sparkContext\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "import numpy as np\n",
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd5e8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar ruta del dataset\n",
    "\n",
    "dataset_path = \"C:/Users/Mario/Documents/1.Estudios/1.UNIR/TFM/Datasets/millionsongsubset/MillionSongSubset\"\n",
    "file_paths = glob.glob(f\"{dataset_path}/**/*.h5\", recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51183695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "|  tempo|loudness| duration|key|mode|time_signature|           song_id|     timbre_mean_0|      timbre_std_0|     timbre_mean_1|      timbre_std_1|     timbre_mean_2|      timbre_std_2|      timbre_mean_3|      timbre_std_3|      timbre_mean_4|      timbre_std_4|     timbre_mean_5|      timbre_std_5|      timbre_mean_6|      timbre_std_6|       timbre_mean_7|      timbre_std_7|       timbre_mean_8|      timbre_std_8|     timbre_mean_9|      timbre_std_9|     timbre_mean_10|     timbre_std_10|     timbre_mean_11|     timbre_std_11|\n",
      "+-------+--------+---------+---+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "| 92.198| -11.197|218.93179|1.0| 0.0|           4.0|SOMZWCG12A8C13C480| 41.51277754891864|4.9445424861636464|15.662936148300714| 61.51997339337335|-5.789681771369724| 58.72095052632763|-0.7898444902162766| 54.47995639684928|-43.234903192584966|32.529316869317284|15.170947476828031| 37.56815900745458| 15.139038105046353| 27.00053438601485|  1.5050010298661167| 35.91573311467799|   6.589570545829048|27.005456594746708| 12.29855509783728| 24.12389233780607|-13.563280123583963|24.510618779447377|  5.442479917610713|17.726602302032287|\n",
      "|121.274|  -9.843|148.03546|6.0| 0.0|           4.0|SOCIWDW12A8C13D406| 43.07103636363632| 5.847454674436288|-4.035390909090912| 33.20336162425165|23.572292727272725|28.860205880821475| 12.923576363636357| 39.90195041394463|-2.5450363636363575|25.291304594384815| 5.052394545454545|29.122329106761285|  9.238150909090923|20.090048693568498|  -4.345974545454544|19.608092828015668|   5.224103636363631| 17.55375405459641|2.9356636363636377|16.797338198264015|-2.7526381818181807| 16.09334278248438| 1.7293963636363623|12.089104898332868|\n",
      "| 100.07|  -9.689|177.47546|8.0| 1.0|           1.0|SOXVLOJ12AB0189215|45.130814946619225| 4.231943455084958| -76.8233256227758| 50.00563550841421| 50.78732918149468| 47.88004425303696| 13.509893238434167| 47.09017558102692| -6.549258007117435| 32.40223142956248|-5.514241992882558| 26.58095510566428| 1.8851779359430594| 28.94485050785504|  -8.963729537366556|24.050853978626776|   3.759862989323842| 21.46642140872314|-5.117953736654802|18.026126999619205|  8.442069395017796|18.973604539307907|  4.477170818505343| 20.07895225861053|\n",
      "|119.293|  -9.013|233.40363|0.0| 1.0|           4.0|SONHOTT12A8C13493C|45.800255785627286|  6.50847469069157|41.148986601705204| 42.41131786925091| 57.59929476248478|33.781197082255545|  5.695314250913528|31.565086811907697| 0.9798928136419023|29.085176299967728|-7.360075517661389|   22.028813052035|-10.917191230207054|22.099069173936204|-0.46227161997563915|17.488866480917718|-0.29941047503045176| 25.67803452823948|-2.340377588306943|15.123002446682744|0.26161632155907566|12.489488957876443|-2.4275980511571236|18.634074343652564|\n",
      "|129.738|  -4.501|209.60608|2.0| 1.0|           4.0|SOFSOCN12A8C143F5D| 50.25155423476959| 4.227845209419915|27.845583952451705|22.555184670048927|47.091303120356585| 39.16986879231001| 11.080035661218442|30.454118747522124|-43.505350668647836| 28.25126808839106|-17.99725260029719|23.275417278162738| -5.284150074294206|20.070394998887185| -11.754643387815745|15.137500984053425|   10.51285141158989|13.721607772597004|2.1924576523031254| 13.08202427621625|  5.448426448737002|14.556299798152125| 1.7045156017830607|10.923347281838689|\n",
      "+-------+--------+---------+---+----+--------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar los features\n",
    "data = []\n",
    "metadata_list = []  # OJO cambiamos el nombre de la lista de metadatos\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "        segments_timbre = GETTERS.get_segments_timbre(h5)\n",
    "        if segments_timbre is None or len(segments_timbre) == 0:\n",
    "            timbre_mean = [0.0] * 12\n",
    "            timbre_std = [0.0] * 12\n",
    "        else:\n",
    "            timbre_mean = np.mean(segments_timbre, axis=0).tolist()\n",
    "            timbre_std = np.std(segments_timbre, axis=0).tolist()\n",
    "\n",
    "        features = {\n",
    "            'tempo': float(GETTERS.get_tempo(h5)),\n",
    "            'loudness': float(GETTERS.get_loudness(h5)),\n",
    "            'duration': float(GETTERS.get_duration(h5)),\n",
    "            'key': float(GETTERS.get_key(h5)),\n",
    "            'mode': float(GETTERS.get_mode(h5)),\n",
    "            'time_signature': float(GETTERS.get_time_signature(h5)),\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8')\n",
    "        }\n",
    "\n",
    "        for i in range(12):\n",
    "            features[f'timbre_mean_{i}'] = timbre_mean[i]\n",
    "            features[f'timbre_std_{i}'] = timbre_std[i]\n",
    "\n",
    "        song_id = GETTERS.get_song_id(h5).decode('utf-8')\n",
    "        title = GETTERS.get_title(h5).decode('utf-8')\n",
    "        artist = GETTERS.get_artist_name(h5).decode('utf-8')\n",
    "\n",
    "        meta = {\n",
    "            'song_id': song_id,\n",
    "            'title': title,\n",
    "            'artist': artist\n",
    "        }\n",
    "\n",
    "        h5.close()\n",
    "\n",
    "        return features, meta\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el fichero: {path} --> {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Recorremos los ficheros uno a uno\n",
    "for path in file_paths:\n",
    "    result, meta = extract_features(path)\n",
    "    if result is not None:\n",
    "        data.append(result)\n",
    "    if meta is not None:\n",
    "        metadata_list.append(meta)\n",
    "\n",
    "# Convertimos las listas a pandas\n",
    "df_pd = pd.DataFrame(data)\n",
    "df_meta = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Guardamos los metadatos\n",
    "df_meta.to_csv(\"songs_metadata.csv\", index=False)\n",
    "\n",
    "# Y lo convertimos a DataFrame de PySpark\n",
    "df = spark.createDataFrame(df_pd)\n",
    "\n",
    "# Visualizamos los primeros registros\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c02db8",
   "metadata": {},
   "source": [
    "#### Entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50d5b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['tempo', 'loudness', 'duration', 'key', 'mode', 'time_signature'] + \\\n",
    "               [f'timbre_mean_{i}' for i in range(12)] + \\\n",
    "               [f'timbre_std_{i}' for i in range(12)]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "\n",
    "kmeans = KMeans(featuresCol='scaled_features',\n",
    "                predictionCol='cluster_id', k=20, seed=42)\n",
    "model = kmeans.fit(df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e23fa",
   "metadata": {},
   "source": [
    "#### Recomendación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2c52cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nueva canción ha sido asignada al cluster: 13\n",
      "+------------------+\n",
      "|           song_id|\n",
      "+------------------+\n",
      "|SOIJXXM12A8C1416D6|\n",
      "|SOTUBVH12AB018D2EC|\n",
      "|SOFGQAP12AB0185946|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Supongamos que tienes ya definido:\n",
    "# model, scaler_model, assembler, df_scaled\n",
    "\n",
    "# 1️⃣ Simulamos la nueva canción\n",
    "new_song = {\n",
    "    'tempo': 120.0,\n",
    "    'loudness': -5.0,\n",
    "    'duration': 210.0,\n",
    "    'key': 2.0,\n",
    "    'mode': 1.0,\n",
    "    'time_signature': 4.0,\n",
    "    'song_id': 'NEW_SONG_ID'\n",
    "}\n",
    "\n",
    "# Simulamos timbre features\n",
    "for i in range(12):\n",
    "    new_song[f'timbre_mean_{i}'] = np.random.uniform(-50, 50)\n",
    "    new_song[f'timbre_std_{i}'] = np.random.uniform(10, 50)\n",
    "\n",
    "# 2️⃣ Convertimos a Spark DataFrame\n",
    "new_song_df_pd = pd.DataFrame([new_song])\n",
    "new_song_df = spark.createDataFrame(new_song_df_pd)\n",
    "\n",
    "# 3️⃣ Ensamblamos y escalamos usando el pipeline original\n",
    "new_song_vec = assembler.transform(new_song_df)\n",
    "new_song_scaled = scaler_model.transform(new_song_vec)\n",
    "\n",
    "# 4️⃣ Hacemos la predicción de cluster\n",
    "new_song_pred = model.transform(new_song_scaled)\n",
    "predicted_cluster = new_song_pred.select(\n",
    "    \"cluster_id\").collect()[0][\"cluster_id\"]\n",
    "\n",
    "print(f\"La nueva canción ha sido asignada al cluster: {predicted_cluster}\")\n",
    "\n",
    "# 5️⃣ Buscamos canciones del dataset original en el mismo cluster\n",
    "\n",
    "# Si no habías guardado los clusters en el df original:\n",
    "df_with_cluster = model.transform(df_scaled)\n",
    "\n",
    "# Filtramos por el mismo cluster\n",
    "similar_songs = df_with_cluster.filter(\n",
    "    df_with_cluster.cluster_id == predicted_cluster)\n",
    "\n",
    "# Mostramos algunas recomendaciones (por ejemplo 3)\n",
    "similar_songs.select(\"song_id\").limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e393b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                song_id             title          artist\n",
      "69   SOIJXXM12A8C1416D6  Rosemary Recalls   Bruce Rowland\n",
      "85   SOTUBVH12AB018D2EC  Indian Love Call    Slim Whitman\n",
      "175  SOFGQAP12AB0185946    Dearly Beloved  Norrie Paramor\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"songs_metadata.csv\")\n",
    "\n",
    "# Supongamos que estos son los song_ids recomendados que has obtenido de tu modelo\n",
    "recommended_song_ids = [row['song_id']\n",
    "                        for row in similar_songs.select(\"song_id\").limit(3).collect()]\n",
    "\n",
    "\n",
    "# Filtramos los metadatos para obtener nombre de la canción y artista\n",
    "recommended_songs = metadata_df[metadata_df['song_id'].isin(\n",
    "    recommended_song_ids)]\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(recommended_songs[['song_id', 'title', 'artist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b23f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
