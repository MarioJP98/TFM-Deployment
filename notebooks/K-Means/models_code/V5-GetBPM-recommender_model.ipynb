{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23eb1629",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "import numpy as np\n",
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739600f",
   "metadata": {},
   "source": [
    "#### 1. Crear la sesión con SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d476e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('XML ETL') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('job.local.dir', 'file:/models/music-recommender-model') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(10).toDF(\"number\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75240a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5647417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.range(10)  # 1 millón de filas\n",
    "print(df_test.count())  # Debe devolver 1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fc4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2 - Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf11b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/opt/bitnami/spark/datasets/millionsong.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657f80da-6152-4608-b4d8-9f2b332716f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+-------+---+----+--------------+--------+------------+\n",
      "|           song_id|               title|              artist|  tempo|key|mode|time_signature|loudness|danceability|\n",
      "+------------------+--------------------+--------------------+-------+---+----+--------------+--------+------------+\n",
      "|SOBLFFE12AF72AA5BA|              Scream|        Adelitas Way| 99.944|  1|   1|             4|  -4.769|         0.0|\n",
      "|SOQPWCR12A6D4FB2A3|A Poor Recipe For...|   Western Addiction|125.475|  7|   1|             4|   -7.24|         0.0|\n",
      "|SOMZWCG12A8C13C480|    I Didn't Mean To|              Casual| 92.198|  1|   0|             4| -11.197|         0.0|\n",
      "|SOJDASC12A8C13EB49|The Lark In The C...|            Alquimia| 41.279|  2|   1|             4| -13.179|         0.0|\n",
      "|SOCIWDW12A8C13D406|           Soul Deep|        The Box Tops|121.274|  6|   0|             4|  -9.843|         0.0|\n",
      "|SOFRDWL12A58A7CEF7|        Hit Da Scene|    Quest_ Pup_ Kevo| 161.99|  6|   0|             4|  -7.227|         0.0|\n",
      "|SOZIULX12A8AE46C39|Come On (Album Ve...|        Super Deluxe|100.812|  5|   1|             4|  -5.232|         0.0|\n",
      "|SOCSNVI12A8C13ECC2|    Heartache People|Big Brother & The...|136.768|  7|   1|             1| -13.225|         0.0|\n",
      "|SOELQTU12AB018949D|           Andalucia|       Stanley Black| 97.623|  2|   1|             3| -14.567|         0.0|\n",
      "|SOXVLOJ12AB0189215|     Amor De Cabaret|    Sonora Santanera| 100.07|  8|   1|             1|  -9.689|         0.0|\n",
      "+------------------+--------------------+--------------------+-------+---+----+--------------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c02db8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d5b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    'tempo',\n",
    "    'loudness',\n",
    "    'danceability',\n",
    "    'key',\n",
    "    'mode',\n",
    "    'time_signature'\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "kmeans = KMeans(\n",
    "    featuresCol='scaled_features',\n",
    "    predictionCol='cluster_id',\n",
    "    k=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "#Cálculo de centroides\n",
    "model = kmeans.fit(df_scaled)\n",
    "\n",
    "#Ver en que centroide cae cada canción\n",
    "df_with_cluster = model.transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8449af34-4ecc-4809-b3e6-f25a682a88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+------------+------+---+----+--------------+--------+------------+--------------------+--------------------+\n",
      "|           song_id| title|      artist| tempo|key|mode|time_signature|loudness|danceability|        features_vec|     scaled_features|\n",
      "+------------------+------+------------+------+---+----+--------------+--------+------------+--------------------+--------------------+\n",
      "|SOBLFFE12AF72AA5BA|Scream|Adelitas Way|99.944|  1|   1|             4|  -4.769|         0.0|[99.9440002441406...|[2.85098281878643...|\n",
      "+------------------+------+------------+------+---+----+--------------+--------+------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e23fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Recomendación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c52cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nueva canción ha sido asignada al cluster: 5\n",
      "+------------------+\n",
      "|           song_id|\n",
      "+------------------+\n",
      "|SOBLFFE12AF72AA5BA|\n",
      "|SONHOTT12A8C13493C|\n",
      "|SOFSOCN12A8C143F5D|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "# Supongamos que tienes ya definido:\n",
    "# model, scaler_model, assembler, df_scaled\n",
    "\n",
    "# Simulamos la nueva canción\n",
    "new_song = {\n",
    "    'tempo': 120.0,\n",
    "    'loudness': -5.0,\n",
    "    'danceability': 1000,\n",
    "    'key': 2.0,\n",
    "    'mode': 1.0,\n",
    "    'time_signature': 4.0,\n",
    "    'song_id': 'NEW_SONG_ID'\n",
    "}\n",
    "\n",
    "\n",
    "# Convertimos a Spark DataFrame\n",
    "new_song_df_pd = pd.DataFrame([new_song])\n",
    "new_song_df = spark.createDataFrame(new_song_df_pd)\n",
    "\n",
    "#  Ensamblamos y escalamos usando el pipeline original\n",
    "new_song_vec = assembler.transform(new_song_df)\n",
    "new_song_scaled = scaler_model.transform(new_song_vec)\n",
    "\n",
    "# Hacemos la predicción de cluster\n",
    "new_song_pred = model.transform(new_song_scaled)\n",
    "predicted_cluster = new_song_pred.select(\n",
    "    \"cluster_id\").collect()[0][\"cluster_id\"]\n",
    "\n",
    "print(f\"La nueva canción ha sido asignada al cluster: {predicted_cluster}\")\n",
    "\n",
    "# Buscamos canciones del dataset original en el mismo cluster\n",
    "\n",
    "# Si no habías guardado los clusters en el df original:\n",
    "df_with_cluster = model.transform(df_scaled)\n",
    "\n",
    "# Filtramos por el mismo cluster\n",
    "similar_songs = df_with_cluster.filter(\n",
    "    df_with_cluster.cluster_id == predicted_cluster)\n",
    "\n",
    "# Mostramos algunas recomendaciones (por ejemplo 3)\n",
    "similar_songs.select(\"song_id\").limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e393b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                song_id                              title           artist\n",
      "0    SOZECOE12AB017E615  Diamonds Are A Girl's Best Friend  Gloria De Haven\n",
      "84   SOPJEXC12A6D4FB529                Hot Dental Supplies      Marga Gomez\n",
      "149  SOJXYFD12A8C143701         The Ferocious O' Flahertys       Joe Heaney\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"songs_metadata.csv\")\n",
    "\n",
    "# Supongamos que estos son los song_ids recomendados que has obtenido de tu modelo\n",
    "recommended_song_ids = [row['song_id']\n",
    "                        for row in similar_songs.select(\"song_id\").limit(3).collect()]\n",
    "\n",
    "\n",
    "# Filtramos los metadatos para obtener nombre de la canción y artista\n",
    "recommended_songs = metadata_df[metadata_df['song_id'].isin(\n",
    "    recommended_song_ids)]\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(recommended_songs[['song_id', 'title', 'artist']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f30e57-1340-4091-8d16-11db1651e830",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99cb0aeb-4053-4ec8-8da4-98415c8bc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creamos el pipeline con los stages que usaste durante el entrenamiento\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Lo ajustamos sobre el dataframe ya escalado\n",
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fd466de-82ae-406b-b55e-efeea7466356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model.write().overwrite().save(\"file:///models/music-recommender-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95514c6-96b1-45e1-bfd6-faf3d5623130",
   "metadata": {},
   "source": [
    "##### Guardar las canciones y a que cluster pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce12f84-da77-4152-a406-ac8364666b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+----+----+--------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+--------------------+--------------------+----------+\n",
      "|  tempo|loudness| duration| key|mode|time_signature|           song_id|               title|              artist|     timbre_mean_0|     timbre_std_0|      timbre_mean_1|      timbre_std_1|      timbre_mean_2|      timbre_std_2|      timbre_mean_3|      timbre_std_3|     timbre_mean_4|      timbre_std_4|      timbre_mean_5|      timbre_std_5|      timbre_mean_6|      timbre_std_6|      timbre_mean_7|      timbre_std_7|     timbre_mean_8|      timbre_std_8|     timbre_mean_9|      timbre_std_9|     timbre_mean_10|     timbre_std_10|    timbre_mean_11|     timbre_std_11|        features_vec|     scaled_features|cluster_id|\n",
      "+-------+--------+---------+----+----+--------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+--------------------+--------------------+----------+\n",
      "|100.296| -14.088|110.88934| 0.0| 0.0|           4.0|SOZECOE12AB017E615|Diamonds Are A Gi...|     Gloria De Haven| 38.62859947643981|5.356287378976018|-101.03716492146597| 64.34570506271035| 27.049188481675404| 58.32669723112375|-1.6972146596858637| 47.09515469765234|21.343900523560208| 34.09278408844165| 15.569753926701566|36.683589576283666| -28.51487696335078|31.933875264135818|  4.463628272251304| 30.15178147998541| 22.10791361256545| 26.86277506521953| 7.504044502617803|26.022637808626587|-1.0602984293193713| 23.72026680716484|12.031910994764385| 23.74240463331764|[100.296,-14.088,...|[2.85058055569665...|         0|\n",
      "|121.968|   -6.01|241.52771|11.0| 1.0|           4.0|SOXNSGB12A8C1396F1|     Gotta Let It Go|          Elva Hsiao|44.535513000000044|5.478194661549649|          34.774147| 47.79820877687145|-3.3423789999999984|  47.2523564346727| 13.590332999999994| 62.78621346850048| 7.593021999999999| 37.52319802156417| 15.685361999999996| 44.94884628229024| 0.2800949999999996|29.522299127642054|  8.233494000000011|28.544652363130364| 2.409291999999998|23.649129849251032|11.965605000000007|23.780341794830765|          -7.616618|26.882983383212462|-9.606646000000014|22.849809052214958|[121.968,-6.01,24...|[3.46653514813362...|         4|\n",
      "|146.913| -11.027|177.57995| 7.0| 0.0|           1.0|SOGXWRE12AC468BE24|      Paint It Black|       Chris Farlowe|44.601593247588404|4.480842603870592|  44.04050803858517|31.206377005139693|   -3.5620884244373|25.259863374364052| -19.09931189710611| 29.28065459926128|4.8730369774919575|27.001562848901884|-14.241966237942117|22.750933922821634| -6.562429260450157|25.513166273507842|-0.6424887459807057|17.992917176935112|14.028934083601287|  19.8787722324577|3.3159839228295893|15.634624963180013|  -5.41135369774919|13.246051473316134| 5.725453376205784|17.314717115634608|[146.913,-11.027,...|[4.17551389067423...|         2|\n",
      "|109.051| -15.586|541.23057| 9.0| 1.0|           4.0|SOIJPPR12A6D4F3945|Requiem Mass_ K. ...|Choir & Great Sym...| 36.03285844748856|8.946115181246396|-109.42725114155256| 43.41483733127256| 103.44874657534241| 57.25723483577007| 3.8192739726027414|41.820336318309735|-23.84902511415523| 43.84637421670602| -9.415046803652974|18.279840506904183|-36.538407534246595|26.305825608843985|-5.5869063926940665|19.973966921591842| 23.75780251141551|31.319411468437018| 5.748410958904105|11.500391933665409| 0.7848915525114164|15.175306566253994|-4.383398401826487|19.765696755908383|[109.051,-15.586,...|[3.09941234126262...|         6|\n",
      "| 87.897|  -9.532|142.86322| 1.0| 1.0|           4.0|SOMUYIL12AB0184EE2|Don't Owe Me Nothin'|Sticky Fingaz & O...| 41.28385321100916|4.325701506084002|  9.338308256880746| 46.54334964716628| 0.4139944954128427|43.906155756893526|  3.009515596330276| 56.59037601487377|-16.11785504587156|27.283615249414268|-3.2823100917431174| 27.26093548498817| -8.804996330275218|21.270836474280532| -6.034921100917432| 33.52847872326639|7.8780844036697255| 23.89795355693433|-4.823576146788991| 23.27838071038045|-0.8821229357798163|19.800566979036457|13.500678899082558|15.082220582302337|[87.897,-9.532,14...|[2.49818017771465...|        18|\n",
      "+-------+--------+---------+----+----+--------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "311933bd-7262-419d-840e-f04945cc38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_export = df_with_cluster.select(\"title\", \"artist\", \"cluster_id\")\n",
    "df_export.write.csv(\"songs_with_cluster\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3283a52-0b9a-4900-9c7a-dae450e40b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.toPandas().to_csv(\"songs_with_cluster.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2d496-ab86-4701-8c28-3d58d71d5d94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Cargar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97416655-e5ad-4943-a375-bbd1213edb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(\"file:///models/music-recommender-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cf9657-20ad-4476-84be-0e5d5445ae47",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "duration does not exist. Available: tempo, loudness, danceability, key, mode, time_signature, track_name, artist_name",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m df_new = spark.createDataFrame(pd.DataFrame([new_song]))\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Predecir cluster directamente con el pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m predicted = \u001b[43mloaded_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m predicted_cluster = predicted.select(\u001b[33m\"\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m\"\u001b[39m).collect()[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLa canción fue asignada al cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_cluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:262\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/pipeline.py:304\u001b[39m, in \u001b[36mPipelineModel._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> DataFrame:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stages:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         dataset = \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:262\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/wrapper.py:398\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: duration does not exist. Available: tempo, loudness, danceability, key, mode, time_signature, track_name, artist_name"
     ]
    }
   ],
   "source": [
    "# Crear nueva canción con timbre simulado\n",
    "new_song = {\n",
    "    'tempo': 120.0,\n",
    "    'loudness': -5.0,\n",
    "    'danceability': 60,\n",
    "    'key': 2.0,\n",
    "    'mode': 1.0,\n",
    "    'time_signature': 4.0,\n",
    "    'track_name': 'New Song',\n",
    "    'artist_name': 'Unknown Artist'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Convertir a DataFrame de Spark\n",
    "df_new = spark.createDataFrame(pd.DataFrame([new_song]))\n",
    "\n",
    "# Predecir cluster directamente con el pipeline\n",
    "predicted = loaded_model.transform(df_new)\n",
    "predicted_cluster = predicted.select(\"cluster_id\").collect()[0][\"cluster_id\"]\n",
    "\n",
    "print(f\"La canción fue asignada al cluster {predicted_cluster}\")\n",
    "\n",
    "df_with_clusters = loaded_model.transform(df_scaled)\n",
    "\n",
    "recommendations = df_with_clusters.filter(\n",
    "    df_with_clusters.cluster_id == predicted_cluster\n",
    ").select(\"track_name\", \"artist_name\").limit(3)\n",
    "\n",
    "recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f0dab7-bf80-45f2-8118-91b9f18bf12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               song_id           title              artist\n",
      "19  SOQBCSM12AC4687CDE  Pete's Crusade  Light Of The World\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"songs_metadata.csv\")\n",
    "\n",
    "# Supongamos que estos son los song_ids recomendados que has obtenido de tu modelo\n",
    "recommended_song_ids = [row['song_id']\n",
    "                        for row in similar_songs.select(\"song_id\").limit(3).collect()]\n",
    "\n",
    "\n",
    "# Filtramos los metadatos para obtener nombre de la canción y artista\n",
    "recommended_songs = metadata_df[metadata_df['song_id'].isin(\n",
    "    recommended_song_ids)]\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(recommended_songs[['song_id', 'title', 'artist']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d0727-e276-4f2f-88aa-6052f84b10d3",
   "metadata": {},
   "source": [
    "### Mejorar el modelo (Métricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00afe62f-a977-4423-a70c-64c3c795b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score (k=20): 0.33837674376824317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(\n",
    "    featuresCol='scaled_features',\n",
    "    predictionCol='cluster_id',\n",
    "    metricName='silhouette',\n",
    "    distanceMeasure='squaredEuclidean'\n",
    ")\n",
    "\n",
    "silhouette = evaluator.evaluate(df_with_cluster)\n",
    "print(f\"Silhouette score (k=20): {silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bfa8daa-e4ee-4c48-8bdb-5a2e00041c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2, silhouette_score=0.3234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3, silhouette_score=0.4142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=4, silhouette_score=0.4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, silhouette_score=0.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=6, silhouette_score=0.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=7, silhouette_score=0.3963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=8, silhouette_score=0.3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=9, silhouette_score=0.3591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10, silhouette_score=0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=11, silhouette_score=0.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=12, silhouette_score=0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=13, silhouette_score=0.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=14, silhouette_score=0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=15, silhouette_score=0.3802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=16, silhouette_score=0.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=17, silhouette_score=0.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=18, silhouette_score=0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=19, silhouette_score=0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=20, silhouette_score=0.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=21, silhouette_score=0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=22, silhouette_score=0.3425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=23, silhouette_score=0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=24, silhouette_score=0.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=25, silhouette_score=0.3451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=26, silhouette_score=0.3366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=27, silhouette_score=0.3369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=28, silhouette_score=0.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=29, silhouette_score=0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1842:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=30, silhouette_score=0.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ks = list(range(2, 31))  # Por ejemplo, de 2 a 30 clusters\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(featuresCol='scaled_features', predictionCol='cluster_id', k=k, seed=42)\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    df_clusters = model.transform(df_scaled)\n",
    "\n",
    "    silhouette = evaluator.evaluate(df_clusters)\n",
    "    print(f\"k={k}, silhouette_score={silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31101-c918-41c7-b5b9-f2a7d109ff81",
   "metadata": {},
   "source": [
    "#### Que faeatures son más significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12933d51-fe16-450e-9de4-1e6c518c6b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1: 0.2542\n",
      "PC2: 0.2323\n",
      "PC3: 0.1915\n",
      "PC4: 0.1630\n",
      "PC5: 0.1590\n",
      "PC6: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 15:23:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=6, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df_scaled)\n",
    "explained = model.explainedVariance.toArray()\n",
    "\n",
    "for i, var in enumerate(explained):\n",
    "    print(f\"PC{i+1}: {var:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891cf787-8034-4edd-b1f8-4e37f4cb6e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
