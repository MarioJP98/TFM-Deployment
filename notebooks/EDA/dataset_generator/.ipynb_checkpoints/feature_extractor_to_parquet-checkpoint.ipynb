{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910be4b0",
   "metadata": {},
   "source": [
    "# Notebook dedicado a extraer las features de los Archivos H5:\n",
    "- Se divide en varios intentos hasta conseguir el m√° eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616efdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import hdf5_getters as GETTERS\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b33a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta base para la letra A\n",
    "dataset_path = r\"C:\\Dataset\\msd_targz\\A.tar\\A\"\n",
    "file_paths = glob.glob(f\"{dataset_path}\\\\**\\\\*.h5\", recursive=True)  # Limitar a los primeros 1000 archivos\n",
    "\n",
    "# O bien usando pathlib (m√°s robusto en general)\n",
    "# file_paths = list(Path(dataset_path).rglob(\"*.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31f3e7",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cad489d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Iterar sobre los archivos\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[1;32m---> 33\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m     35\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     h5 \u001b[38;5;241m=\u001b[39m GETTERS\u001b[38;5;241m.\u001b[39mopen_h5_file_read(path)\n\u001b[0;32m     11\u001b[0m     features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msong_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mGETTERS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_song_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: GETTERS\u001b[38;5;241m.\u001b[39mget_title(h5)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m: GETTERS\u001b[38;5;241m.\u001b[39mget_artist_name(h5)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_tempo(h5)),\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_key(h5)),\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_mode(h5)),\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_signature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_time_signature(h5)),\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloudness\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_loudness(h5)),\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdanceability\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(GETTERS\u001b[38;5;241m.\u001b[39mget_danceability(h5))\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     23\u001b[0m     h5\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\Mario\\OneDrive - UNIR\\1.TFM\\repo\\TFM-Deployment\\docker\\spark\\notebooks\\hdf5_getters.py:125\u001b[0m, in \u001b[0;36mget_song_id\u001b[1;34m(h5, songidx)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_song_id\u001b[39m(h5,songidx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    Get song id from a HDF5 song file, by default the first song in it\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msongs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msong_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43msongidx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\site-packages\\tables\\table.py:3534\u001b[0m, in \u001b[0;36mColumn.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3532\u001b[0m         key \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mnrows\n\u001b[0;32m   3533\u001b[0m     (start, stop, step) \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39m_process_range(key, key \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 3534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpathname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   3535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   3536\u001b[0m     (start, stop, step) \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39m_process_range(\n\u001b[0;32m   3537\u001b[0m         key\u001b[38;5;241m.\u001b[39mstart, key\u001b[38;5;241m.\u001b[39mstop, key\u001b[38;5;241m.\u001b[39mstep)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\site-packages\\tables\\table.py:1984\u001b[0m, in \u001b[0;36mTable.read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m   1981\u001b[0m start, stop, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_range(start, stop, step,\n\u001b[0;32m   1982\u001b[0m                                         warn_negstep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1984\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m internal_to_flavor(arr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflavor)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\site-packages\\tables\\table.py:1902\u001b[0m, in \u001b[0;36mTable._read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_field_name(result, start, stop, step, field)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m select_field:\n\u001b[0;32m   1905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[select_field]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lista para los features\n",
    "data = []\n",
    "\n",
    "# Funci√≥n para extraer\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "\n",
    "        features = {\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8'),\n",
    "            'title': GETTERS.get_title(h5).decode('utf-8'),\n",
    "            'artist': GETTERS.get_artist_name(h5).decode('utf-8'),\n",
    "            'tempo': float(GETTERS.get_tempo(h5)),\n",
    "            'key': float(GETTERS.get_key(h5)),\n",
    "            'mode': float(GETTERS.get_mode(h5)),\n",
    "            'time_signature': int(GETTERS.get_time_signature(h5)),\n",
    "            'loudness': float(GETTERS.get_loudness(h5)),\n",
    "            'danceability': float(GETTERS.get_danceability(h5))\n",
    "        }\n",
    "\n",
    "        h5.close()\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error con archivo {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Iterar sobre los archivos\n",
    "for path in file_paths:\n",
    "    result = extract_features(path)\n",
    "    if result:\n",
    "        data.append(result)\n",
    "\n",
    "# Crear DataFrame\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Guardar como parquet\n",
    "df_pd.to_parquet(r\"C:\\Dataset\\parquet\\A.parquet\", index=False)\n",
    "\n",
    "print(\"Proceso completado con √©xito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d073043",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a796b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:   3%|‚ñé         | 1009/39100 [00:18<12:08, 52.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_0.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:   5%|‚ñå         | 2010/39100 [00:36<11:12, 55.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_1.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:   8%|‚ñä         | 3009/39100 [00:55<10:41, 56.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_2.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  10%|‚ñà         | 4005/39100 [01:13<10:34, 55.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_3.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  13%|‚ñà‚ñé        | 5006/39100 [01:33<11:51, 47.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_4.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  15%|‚ñà‚ñå        | 6008/39100 [01:53<09:59, 55.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_5.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  18%|‚ñà‚ñä        | 7005/39100 [02:11<09:43, 54.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_6.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  20%|‚ñà‚ñà        | 8009/39100 [02:29<09:38, 53.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_7.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  23%|‚ñà‚ñà‚ñé       | 9006/39100 [02:47<10:34, 47.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_8.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  26%|‚ñà‚ñà‚ñå       | 10009/39100 [03:05<08:27, 57.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_9.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  28%|‚ñà‚ñà‚ñä       | 11005/39100 [03:23<08:22, 55.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_10.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  31%|‚ñà‚ñà‚ñà       | 12005/39100 [03:40<08:05, 55.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_11.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  33%|‚ñà‚ñà‚ñà‚ñé      | 13008/39100 [03:58<07:54, 55.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_12.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  36%|‚ñà‚ñà‚ñà‚ñå      | 14009/39100 [04:16<07:24, 56.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_13.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  38%|‚ñà‚ñà‚ñà‚ñä      | 15007/39100 [04:34<07:03, 56.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_14.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  41%|‚ñà‚ñà‚ñà‚ñà      | 16005/39100 [04:52<07:03, 54.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_15.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17005/39100 [05:11<07:18, 50.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_16.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 18008/39100 [05:30<06:21, 55.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_17.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 19009/39100 [05:49<06:44, 49.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_18.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 20005/39100 [06:08<06:05, 52.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_19.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21009/39100 [06:26<05:20, 56.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_20.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 22009/39100 [06:45<05:08, 55.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_21.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 23008/39100 [07:06<05:15, 51.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_22.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 24006/39100 [07:25<06:12, 40.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_23.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 25006/39100 [07:48<05:49, 40.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_24.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 26007/39100 [08:08<04:11, 52.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_25.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 27007/39100 [08:26<03:32, 57.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_26.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 28009/39100 [08:46<03:45, 49.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_27.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 29008/39100 [09:04<03:10, 52.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_28.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 30008/39100 [09:22<02:43, 55.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_29.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 31008/39100 [09:40<02:51, 47.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_30.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 32008/39100 [09:58<02:06, 56.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_31.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 33009/39100 [10:17<02:09, 46.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_32.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 34008/39100 [10:38<01:47, 47.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_33.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 35007/39100 [11:00<01:27, 46.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_34.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 36009/39100 [11:21<01:06, 46.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_35.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 37004/39100 [11:42<00:49, 42.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_36.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 38010/39100 [12:04<00:23, 47.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_37.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 39008/39100 [12:25<00:01, 46.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Dataset\\parquet\\batches\\features_batch_38.parquet (1000 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39100/39100 [12:27<00:00, 52.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado (√∫ltimo): C:\\Dataset\\parquet\\batches\\features_batch_39.parquet (100 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ruta base\n",
    "dataset_path = r\"C:\\Dataset\\msd_targz\\A.tar\\A\"\n",
    "file_paths = glob.glob(f\"{dataset_path}\\\\**\\\\*.h5\", recursive=True)\n",
    "\n",
    "# Par√°metros\n",
    "batch_size = 1000\n",
    "batch_data = []\n",
    "batch_counter = 0\n",
    "save_dir = r\"C:\\Dataset\\parquet\\batches\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Funci√≥n para extraer features de un archivo .h5\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "        features = {\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8'),\n",
    "            'title': GETTERS.get_title(h5).decode('utf-8'),\n",
    "            'artist': GETTERS.get_artist_name(h5).decode('utf-8'),\n",
    "            'tempo': float(GETTERS.get_tempo(h5)),\n",
    "            'key': float(GETTERS.get_key(h5)),\n",
    "            'mode': float(GETTERS.get_mode(h5)),\n",
    "            'time_signature': int(GETTERS.get_time_signature(h5)),\n",
    "            'loudness': float(GETTERS.get_loudness(h5)),\n",
    "            'danceability': float(GETTERS.get_danceability(h5))\n",
    "        }\n",
    "        h5.close()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error con archivo {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Procesamiento con barra de progreso\n",
    "for i, path in enumerate(tqdm(file_paths, desc=\"Procesando archivos\")):\n",
    "    result = extract_features(path)\n",
    "    if result:\n",
    "        batch_data.append(result)\n",
    "\n",
    "    if len(batch_data) >= batch_size:\n",
    "        df_batch = pd.DataFrame(batch_data)\n",
    "        output_path = os.path.join(\n",
    "            save_dir, f\"features_batch_{batch_counter}.parquet\")\n",
    "        df_batch.to_parquet(output_path, compression=\"snappy\", index=False)\n",
    "        print(f\"Guardado: {output_path} ({len(df_batch)} canciones)\")\n",
    "        batch_data = []\n",
    "        batch_counter += 1\n",
    "\n",
    "# Guardar el √∫ltimo batch si queda algo\n",
    "if batch_data:\n",
    "    df_batch = pd.DataFrame(batch_data)\n",
    "    output_path = os.path.join(\n",
    "        save_dir, f\"features_batch_{batch_counter}.parquet\")\n",
    "    df_batch.to_parquet(output_path, compression=\"snappy\", index=False)\n",
    "    print(f\"Guardado (√∫ltimo): {output_path} ({len(df_batch)} canciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "258286d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset final guardado en: C:\\Dataset\\parquet\\features_A_final.parquet\n",
      "‚û°Ô∏è N√∫mero total de canciones: 39100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Ruta donde guardaste los batches\n",
    "input_dir = r\"C:\\Dataset\\parquet\\batches\"\n",
    "output_file = r\"C:\\Dataset\\parquet\\features_A_final.parquet\"\n",
    "\n",
    "# Obtener todos los archivos .parquet de la carpeta\n",
    "parquet_files = glob.glob(os.path.join(input_dir, \"*.parquet\"))\n",
    "\n",
    "# Leer y concatenar todos los DataFrames\n",
    "dataframes = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenar todos en uno solo\n",
    "df_final = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Guardar el resultado final\n",
    "df_final.to_parquet(output_file, index=False, compression=\"snappy\")\n",
    "\n",
    "print(f\"‚úÖ Dataset final guardado en: {output_file}\")\n",
    "print(f\"‚û°Ô∏è N√∫mero total de canciones: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5432c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "              song_id                             title             artist  \\\n",
      "0  SOBLFFE12AF72AA5BA                            Scream       Adelitas Way   \n",
      "1  SOQPWCR12A6D4FB2A3  A Poor Recipe For Civic Cohesion  Western Addiction   \n",
      "2  SOMZWCG12A8C13C480                  I Didn't Mean To             Casual   \n",
      "3  SOJDASC12A8C13EB49         The Lark In The Clear Air           Alquimia   \n",
      "4  SOCIWDW12A8C13D406                         Soul Deep       The Box Tops   \n",
      "\n",
      "     tempo  key  mode  time_signature  loudness  danceability  \n",
      "0   99.944  1.0   1.0               4    -4.769           0.0  \n",
      "1  125.475  7.0   1.0               4    -7.240           0.0  \n",
      "2   92.198  1.0   0.0               4   -11.197           0.0  \n",
      "3   41.279  2.0   1.0               4   -13.179           0.0  \n",
      "4  121.274  6.0   0.0               4    -9.843           0.0  \n",
      "\n",
      "Informaci√≥n general del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39100 entries, 0 to 39099\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   song_id         39100 non-null  object \n",
      " 1   title           39100 non-null  object \n",
      " 2   artist          39100 non-null  object \n",
      " 3   tempo           39100 non-null  float64\n",
      " 4   key             39100 non-null  float64\n",
      " 5   mode            39100 non-null  float64\n",
      " 6   time_signature  39100 non-null  int64  \n",
      " 7   loudness        39100 non-null  float64\n",
      " 8   danceability    39100 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(3)\n",
      "memory usage: 2.7+ MB\n",
      "None\n",
      "\n",
      "Columnas disponibles:\n",
      "['song_id', 'title', 'artist', 'tempo', 'key', 'mode', 'time_signature', 'loudness', 'danceability']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo .parquet generado\n",
    "parquet_path = r\"C:\\Dataset\\parquet\\A.parquet\"\n",
    "\n",
    "# Cargar el DataFrame desde Parquet\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Ver las primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Ver informaci√≥n general\n",
    "print(\"\\nInformaci√≥n general del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Ver columnas\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d446c42",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Procesando batch 1 (5000 archivos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:29<00:00, 56.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado: C:\\Dataset\\parquet\\batches_A\\batch_1.parquet (5000 canciones)\n",
      "\n",
      "üîÑ Procesando batch 2 (5000 archivos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:36<00:00, 51.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado: C:\\Dataset\\parquet\\batches_A\\batch_2.parquet (5000 canciones)\n",
      "\n",
      "üîÑ Procesando batch 3 (5000 archivos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:35<00:00, 52.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado: C:\\Dataset\\parquet\\batches_A\\batch_3.parquet (5000 canciones)\n",
      "\n",
      "üîÑ Procesando batch 4 (5000 archivos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:23<00:00, 59.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado: C:\\Dataset\\parquet\\batches_A\\batch_4.parquet (5000 canciones)\n",
      "\n",
      "üîÑ Procesando batch 5 (5000 archivos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  10%|‚ñà         | 503/5000 [00:09<01:24, 53.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---------------- CONFIGURACI√ìN ----------------\n",
    "dataset_path = r\"C:\\Dataset\\msd_targz\\A.tar\\A\"\n",
    "output_dir = r\"C:\\Dataset\\parquet\\batches_A\"\n",
    "batch_size = 5000\n",
    "num_threads = 12\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Obtener archivos .h5\n",
    "file_paths = glob.glob(f\"{dataset_path}\\\\**\\\\*.h5\", recursive=True)\n",
    "\n",
    "# ---------------- FUNCI√ìN DE EXTRACCI√ìN ----------------\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "        features = {\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8'),\n",
    "            'title': GETTERS.get_title(h5).decode('utf-8'),\n",
    "            'artist': GETTERS.get_artist_name(h5).decode('utf-8'),\n",
    "            'tempo': np.float32(GETTERS.get_tempo(h5)),\n",
    "            'key': np.int8(GETTERS.get_key(h5)),\n",
    "            'mode': np.int8(GETTERS.get_mode(h5)),\n",
    "            'time_signature': np.int8(GETTERS.get_time_signature(h5)),\n",
    "            'loudness': np.float32(GETTERS.get_loudness(h5)),\n",
    "            'danceability': np.float32(GETTERS.get_danceability(h5))\n",
    "        }\n",
    "        h5.close()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con archivo {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ---------------- PROCESAMIENTO EN LOTES ----------------\n",
    "for i in range(0, len(file_paths), batch_size):\n",
    "    batch_paths = file_paths[i:i + batch_size]\n",
    "    batch_data = []\n",
    "\n",
    "    print(\n",
    "        f\"\\nüîÑ Procesando batch {i // batch_size + 1} ({len(batch_paths)} archivos)\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(extract_features, path)\n",
    "                   for path in batch_paths]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {i // batch_size + 1}\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                batch_data.append(result)\n",
    "\n",
    "    if batch_data:\n",
    "        df_batch = pd.DataFrame(batch_data)\n",
    "        batch_file = os.path.join(\n",
    "            output_dir, f\"batch_{i // batch_size + 1}.parquet\")\n",
    "        df_batch.to_parquet(batch_file, index=False, compression=\"snappy\")\n",
    "        print(f\"‚úÖ Guardado: {batch_file} ({len(df_batch)} canciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246bed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unificando batches:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unificando batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 43.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset final guardado en: C:\\Dataset\\parquet\\B.parquet (38265 canciones)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Ruta de los batches y output final\n",
    "input_dir = r\"C:\\Dataset\\parquet\\batches_B\"\n",
    "output_file = r\"C:\\Dataset\\parquet\\B.parquet\"\n",
    "\n",
    "# Buscar todos los .parquet generados por batch\n",
    "parquet_files = sorted(glob.glob(os.path.join(input_dir, \"*.parquet\")))\n",
    "\n",
    "# Leer y concatenar\n",
    "dataframes = []\n",
    "for file in tqdm(parquet_files, desc=\"Unificando batches\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "df_final = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Guardar el √∫nico archivo final\n",
    "df_final.to_parquet(output_file, index=False, compression=\"snappy\")\n",
    "print(\n",
    "    f\"\\n‚úÖ Dataset final guardado en: {output_file} ({len(df_final)} canciones)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf959e",
   "metadata": {},
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068ac99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Procesando batch 1 (5000 archivos)\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîÑ Procesando batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m archivos)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 55\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_paths\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     56\u001b[0m                         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch_paths),\n\u001b[0;32m     57\u001b[0m                         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     59\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_data:\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\concurrent\\futures\\process.py:766\u001b[0m, in \u001b[0;36mProcessPoolExecutor.map\u001b[1;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunksize must be >= 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 766\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_process_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m_get_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\concurrent\\futures\\_base.py:610\u001b[0m, in \u001b[0;36mExecutor.map\u001b[1;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 610\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\concurrent\\futures\\_base.py:610\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 610\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Mario\\.conda\\envs\\TFM-UNIR\\lib\\concurrent\\futures\\process.py:720\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_lock:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot schedule new futures after shutdown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "# ---------------- CONFIGURACI√ìN ----------------\n",
    "dataset_path = r\"C:\\Dataset\\msd_targz\\B.tar\\B\"\n",
    "output_dir = r\"C:\\Dataset\\parquet\\batches_B\"\n",
    "batch_size = 5000\n",
    "num_workers = min(12, multiprocessing.cpu_count())  # Usa hasta 12 cores\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Obtener archivos .h5\n",
    "file_paths = glob.glob(f\"{dataset_path}\\\\**\\\\*.h5\", recursive=True)\n",
    "\n",
    "# ---------------- FUNCI√ìN DE EXTRACCI√ìN ----------------\n",
    "\n",
    "\n",
    "def extract_features(path):\n",
    "    import hdf5_getters as GETTERS\n",
    "    import numpy as np\n",
    "    try:\n",
    "        h5 = GETTERS.open_h5_file_read(path)\n",
    "        features = {\n",
    "            'song_id': GETTERS.get_song_id(h5).decode('utf-8'),\n",
    "            'title': GETTERS.get_title(h5).decode('utf-8'),\n",
    "            'artist': GETTERS.get_artist_name(h5).decode('utf-8'),\n",
    "            'tempo': np.float32(GETTERS.get_tempo(h5)),\n",
    "            'key': np.int8(GETTERS.get_key(h5)),\n",
    "            'mode': np.int8(GETTERS.get_mode(h5)),\n",
    "            'time_signature': np.int8(GETTERS.get_time_signature(h5)),\n",
    "            'loudness': np.float32(GETTERS.get_loudness(h5)),\n",
    "            'danceability': np.float32(GETTERS.get_danceability(h5))\n",
    "        }\n",
    "        h5.close()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con archivo {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ---------------- PROCESAMIENTO EN LOTES ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(0, len(file_paths), batch_size):\n",
    "        batch_paths = file_paths[i:i + batch_size]\n",
    "        print(\n",
    "            f\"\\nüîÑ Procesando batch {i // batch_size + 1} ({len(batch_paths)} archivos)\")\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(extract_features, batch_paths),\n",
    "                                total=len(batch_paths),\n",
    "                                desc=f\"Batch {i // batch_size + 1}\"))\n",
    "\n",
    "        batch_data = [r for r in results if r]\n",
    "\n",
    "        if batch_data:\n",
    "            df_batch = pd.DataFrame(batch_data)\n",
    "            batch_file = os.path.join(\n",
    "                output_dir, f\"batch_{i // batch_size + 1}.parquet\")\n",
    "            df_batch.to_parquet(batch_file, index=False, compression=\"snappy\")\n",
    "            print(f\"‚úÖ Guardado: {batch_file} ({len(df_batch)} canciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48da658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Letra D completada ‚Üí C:\\Dataset\\parquet\\D.parquet (38825 canciones)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# batches_dir = r\"C:\\Dataset\\parquet\\batches_E\"\n",
    "# output_file = r\"C:\\Dataset\\parquet\\E.parquet\"\n",
    "\n",
    "# batch_files = glob.glob(os.path.join(batches_dir, \"*.parquet\"))\n",
    "# all_dfs = [pd.read_parquet(f) for f in batch_files]\n",
    "# df_merged = pd.concat(all_dfs, ignore_index=True)\n",
    "# df_merged.to_parquet(output_file, index=False, compression=\"snappy\")\n",
    "\n",
    "# print(f\"üéâ Letra A completada ‚Üí {output_file} ({len(df_merged)} canciones)\")\n",
    "\n",
    "for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "    batches_dir = f\"C:\\\\Dataset\\\\parquet\\\\batches_{letter}\"\n",
    "    output_file = f\"C:\\\\Dataset\\\\parquet\\\\{letter}.parquet\"\n",
    "\n",
    "    batch_files = glob.glob(os.path.join(batches_dir, \"*.parquet\"))\n",
    "    all_dfs = [pd.read_parquet(f) for f in batch_files]\n",
    "    df_merged = pd.concat(all_dfs, ignore_index=True)\n",
    "    df_merged.to_parquet(output_file, index=False, compression=\"snappy\")\n",
    "\n",
    "    print(f\"üéâ Letra {letter} completada ‚Üí {output_file} ({len(df_merged)} canciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d62a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "              song_id                                   title  \\\n",
      "0  SOHODMI12AB0188D26  O Come All Ye Faithful (Album Version)   \n",
      "1  SOPQYKD12A6701DECA                                   Games   \n",
      "2  SOYXMUP12A81C1FD73                               Harmonize   \n",
      "3  SOYTKKR12AF72A5DA5  Everybody Cares_ Everybody Understands   \n",
      "4  SOMPLDW12A8C13883A                             Silver Star   \n",
      "\n",
      "                   artist       tempo  key  mode  time_signature  loudness  \\\n",
      "0              Brenda Lee  103.718002    2     1               4   -13.411   \n",
      "1  Big L / Sadat X / Guru   87.955002   11     0               4   -11.524   \n",
      "2          Spirit Catcher  125.945000   11     0               3    -9.578   \n",
      "3           Elliott Smith  116.129997    2     1               4    -8.396   \n",
      "4              Chuck Loeb  169.977997    1     1               4    -9.517   \n",
      "\n",
      "   danceability  \n",
      "0           0.0  \n",
      "1           0.0  \n",
      "2           0.0  \n",
      "3           0.0  \n",
      "4           0.0  \n",
      "\n",
      "Informaci√≥n general del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38611 entries, 0 to 38610\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   song_id         38611 non-null  object \n",
      " 1   title           38611 non-null  object \n",
      " 2   artist          38611 non-null  object \n",
      " 3   tempo           38611 non-null  float32\n",
      " 4   key             38611 non-null  int8   \n",
      " 5   mode            38611 non-null  int8   \n",
      " 6   time_signature  38611 non-null  int8   \n",
      " 7   loudness        38611 non-null  float32\n",
      " 8   danceability    38611 non-null  float32\n",
      "dtypes: float32(3), int8(3), object(3)\n",
      "memory usage: 1.4+ MB\n",
      "None\n",
      "\n",
      "Columnas disponibles:\n",
      "['song_id', 'title', 'artist', 'tempo', 'key', 'mode', 'time_signature', 'loudness', 'danceability']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo .parquet generado\n",
    "parquet_path = r\"C:\\Dataset\\parquet\\C.parquet\"\n",
    "\n",
    "# Cargar el DataFrame desde Parquet\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Ver las primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Ver informaci√≥n general\n",
    "print(\"\\nInformaci√≥n general del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Ver columnas\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7bca9",
   "metadata": {},
   "source": [
    "#### Generar el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81aaf2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Total de canciones en el dataset: 1000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Ruta a todos los archivos .parquet por letra (A.parquet, ..., Z.parquet)\n",
    "parquet_files = glob.glob(r\"C:\\Dataset\\parquet\\*.parquet\")\n",
    "\n",
    "# Leer y concatenar todos los .parquet\n",
    "df = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "\n",
    "# Guardar en un √∫nico archivo unificado (opcional)\n",
    "df.to_parquet(r\"C:\\Dataset\\parquet\\full_dataset.parquet\",\n",
    "              index=False, compression='snappy')\n",
    "\n",
    "# Mostrar el n√∫mero total de canciones\n",
    "print(f\"üéµ Total de canciones en el dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef488107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
